---
apiVersion: v1
kind: ConfigMap
metadata:
  name: deepseek-load-test-script
  namespace: deepseek
data:
  load-test.sh: |
    #!/usr/bin/env bash
    # Fixed GPU Load Test for DeepSeek - Based on working endpoint format
    set -euo pipefail

    ENDPOINT=${1:-"http://127.0.0.1:8000"}
    MAX_CONCURRENT=${2:-100}
    DURATION_MINUTES=${3:-10}

    echo "üî• GPU Load Test for DeepSeek"
    echo "=============================="
    echo "üéØ Endpoint: $ENDPOINT/v1/chat/completions"
    echo "‚ö° Max Concurrent: $MAX_CONCURRENT requests"
    echo "‚è∞ Duration: $DURATION_MINUTES minutes"
    echo ""

    # Global counters
    SUCCESS_COUNT=0
    ERROR_COUNT=0
    ACTIVE_COUNT=0

    ULTRA_INTENSIVE_PROMPTS=(
      # ‚Ä¶ your array of intensive prompts ‚Ä¶
    )

    make_intensive_request() {
      local request_id=$1
      local prompt_index=$((RANDOM % ${#ULTRA_INTENSIVE_PROMPTS[@]}))
      local prompt="${ULTRA_INTENSIVE_PROMPTS[$prompt_index]}"

      ((ACTIVE_COUNT++))
      local start_time=$(date +%s)

      response=$(curl -s -w "%{http_code}" -X POST "$ENDPOINT/v1/chat/completions" \
        -H "Content-Type: application/json" \
        --max-time 600 \
        --connect-timeout 15 \
        -d "{
          \"messages\": [
            {\"role\":\"system\",\"content\":\"You are an advanced computational engine‚Ä¶\"},
            {\"role\":\"user\",\"content\":\"$prompt\"}
          ],
          \"temperature\":0.8,
          \"max_tokens\":1800,
          \"stream\":false,
          \"top_p\":0.95
        }")

      local end_time=$(date +%s)
      local duration=$((end_time - start_time))
      local http_code="${response: -3}"

      ((ACTIVE_COUNT--))

      if [[ "$http_code" == "200" ]]; then
        ((SUCCESS_COUNT++))
        echo "‚úÖ Intensive Request $request_id completed in ${duration}s [Success: $SUCCESS_COUNT, Active: $ACTIVE_COUNT]"
      else
        ((ERROR_COUNT++))
        echo "‚ùå Request $request_id failed with HTTP $http_code [Errors: $ERROR_COUNT, Active: $ACTIVE_COUNT]"
        echo "   Preview: ${response%???}" | head -c 200
      fi
    }

    # Test endpoint connectivity once
    echo "üîç Testing endpoint‚Ä¶"
    test_code=$(curl -s -w "%{http_code}" -X POST "$ENDPOINT/v1/chat/completions" \
      -H "Content-Type: application/json" \
      --max-time 30 \
      -d '{"messages":[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"Say hello in exactly 5 words."}],"temperature":0.1,"max_tokens":20,"stream":false}')

    if [[ "${test_code: -3}" != "200" ]]; then
      echo "‚ùå Endpoint test failed (HTTP ${test_code: -3})"
      exit 1
    fi

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deepseek-vllm-vllm-chart
  namespace: deepseek
  labels:
    app.kubernetes.io/name: vllm-chart
    app.kubernetes.io/instance: deepseek-vllm
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: vllm-chart
      app.kubernetes.io/instance: deepseek-vllm
  template:
    metadata:
      labels:
        app.kubernetes.io/name: vllm-chart
        app.kubernetes.io/instance: deepseek-vllm
    spec:
      containers:
      - name: vllm-chart
        image: vllm/vllm-openai:latest
        command: ["/bin/sh","-c"]
        args:
        - vllm serve TinyLlama/TinyLlama-1.1B-Chat-v1.0 --max-model-len 2048 --dtype=half
        ports:
        - containerPort: 8000
        resources:
          limits:
            cpu: "2"
            memory: "10Gi"
            nvidia.com/gpu: "1"
          requests:
            cpu: "1"
            memory: "5Gi"
            nvidia.com/gpu: "1"
        volumeMounts:
        - name: cache-volume
          mountPath: /root/.cache/huggingface
        - name: shm
          mountPath: /dev/shm

      - name: load-tester
        # Use a tiny Alpine image, install bash+curl at startup
        image: alpine:3.18
        command: ["/bin/sh","-c"]
        args:
        - |
          apk add --no-cache bash curl;
          chmod +x /scripts/load-test.sh;
          # loop forever, pointing at the vLLM service DNS
          while true; do
            /usr/bin/env bash /scripts/load-test.sh http://deepseek-vllm-vllm-chart.deepseek.svc.cluster.local 100 10;
          done
        volumeMounts:
        - name: load-test-script
          mountPath: /scripts

      volumes:
      - name: cache-volume
        hostPath:
          path: /tmp/tinyllama
          type: DirectoryOrCreate
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: 2Gi
      - name: load-test-script
        configMap:
          name: deepseek-load-test-script
          defaultMode: 0755

      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule

---
apiVersion: v1
kind: Service
metadata:
  name: deepseek-vllm-vllm-chart
  namespace: deepseek
  labels:
    app.kubernetes.io/name: vllm-chart
    app.kubernetes.io/instance: deepseek-vllm
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: vllm-chart
    app.kubernetes.io/instance: deepseek-vllm
  ports:
  - name: http
    port: 80
    targetPort: 8000
